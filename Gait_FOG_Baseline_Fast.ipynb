{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d77edbf",
   "metadata": {},
   "source": [
    "# Gait FOG Detection — Fast Baseline (with robust `events.csv` support)\n",
    "\n",
    "This notebook builds windowed features from your **gait** CSVs and trains a simple classifier for **FOG detection**.\n",
    "It includes fast-mode controls and robust handling of your `events.csv` schema (e.g., `id`, `init`, `completion`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "633f8520",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy OK\n",
      "pandas OK\n",
      "scipy OK\n",
      "sklearn OK\n",
      "tqdm OK\n"
     ]
    }
   ],
   "source": [
    "# --- Environment & deps ---\n",
    "import importlib, sys, subprocess\n",
    "\n",
    "def ensure(pkg, pip_name=None):\n",
    "    name = pip_name or pkg\n",
    "    try:\n",
    "        importlib.import_module(pkg)\n",
    "        print(f\"{pkg} OK\")\n",
    "    except ImportError:\n",
    "        print(f\"Installing {name} ...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", name])\n",
    "        importlib.invalidate_caches()\n",
    "        importlib.import_module(pkg)\n",
    "        print(f\"{pkg} installed\")\n",
    "\n",
    "ensure(\"numpy\")\n",
    "ensure(\"pandas\")\n",
    "ensure(\"scipy\")\n",
    "ensure(\"sklearn\", \"scikit-learn\")\n",
    "ensure(\"tqdm\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9f4a351f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GAIT_DIR -> C:\\Users\\muham\\_Projects\\PD New\\data\\gait\n"
     ]
    }
   ],
   "source": [
    "# --- Imports ---\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# --- Locate GAIT directory automatically (edit POSSIBLE_ROOTS if needed) ---\n",
    "POSSIBLE_ROOTS = [\n",
    "    Path(r\"C:\\Users\\muham\\_Projects\\PD New\"),\n",
    "    Path(r\"C:\\Users\\muham\\_Projects\\PD\"),\n",
    "    Path.cwd(),\n",
    "]\n",
    "\n",
    "def find_gait_dir():\n",
    "    for base in POSSIBLE_ROOTS:\n",
    "        for cand in [base / \"data\" / \"gait\", base / \"gait\"]:\n",
    "            if cand.exists():\n",
    "                return cand\n",
    "    return None\n",
    "\n",
    "GAIT_DIR = find_gait_dir()\n",
    "print(\"GAIT_DIR ->\", GAIT_DIR)\n",
    "assert GAIT_DIR is not None, \"Couldn't locate gait folder. Set GAIT_DIR manually to your '.../data/gait' path.\"\n",
    "\n",
    "# --- Fast mode and signal config ---\n",
    "FS = 100          # default Hz if no time column\n",
    "WIN_S = 2.0       # seconds\n",
    "HOP_S = 0.5       # seconds\n",
    "\n",
    "# ===== Speed knobs =====\n",
    "FAST_MODE = True               # quick run\n",
    "DOWN_SAMPLE_TO = 50            # Hz target if a time column exists (try 25 for faster)\n",
    "MAX_RECORDINGS = 120           # limit # of training recordings processed (raise later)\n",
    "MAX_WINDOWS_PER_REC = 500      # cap windows per recording\n",
    "USE_FREQ_FEATS = False         # time-domain stats only (faster). Set True later for accuracy.\n",
    "PRINT_EVERY = 10               # status every N recordings\n",
    "NUMERIC_LIMIT = 128            # limit number of channels to keep feature size manageable\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09077a39",
   "metadata": {},
   "source": [
    "## Manifest (list of recordings)\n",
    "If you already created a `gait_manifest.csv`, we'll load it. Otherwise, we build a simple manifest by scanning `train/*/*.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0a2cfdeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[manifest] Loaded -> C:\\Users\\muham\\_Projects\\PD New\\data\\manifests\\gait_manifest.csv (rows=970)\n",
      "[manifest] train rows: 970\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>recording_id</th>\n",
       "      <th>split</th>\n",
       "      <th>source</th>\n",
       "      <th>subject_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C:\\Users\\muham\\_Projects\\PD New\\data\\gait\\trai...</td>\n",
       "      <td>02ea782681</td>\n",
       "      <td>train</td>\n",
       "      <td>defog</td>\n",
       "      <td>02ea782681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C:\\Users\\muham\\_Projects\\PD New\\data\\gait\\trai...</td>\n",
       "      <td>06414383cf</td>\n",
       "      <td>train</td>\n",
       "      <td>defog</td>\n",
       "      <td>06414383cf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C:\\Users\\muham\\_Projects\\PD New\\data\\gait\\trai...</td>\n",
       "      <td>092b4c1819</td>\n",
       "      <td>train</td>\n",
       "      <td>defog</td>\n",
       "      <td>092b4c1819</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                path recording_id  split  \\\n",
       "0  C:\\Users\\muham\\_Projects\\PD New\\data\\gait\\trai...   02ea782681  train   \n",
       "1  C:\\Users\\muham\\_Projects\\PD New\\data\\gait\\trai...   06414383cf  train   \n",
       "2  C:\\Users\\muham\\_Projects\\PD New\\data\\gait\\trai...   092b4c1819  train   \n",
       "\n",
       "  source  subject_id  \n",
       "0  defog  02ea782681  \n",
       "1  defog  06414383cf  \n",
       "2  defog  092b4c1819  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MANIFESTS_DIR = GAIT_DIR.parent / \"manifests\"\n",
    "MANIFESTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "MANIFEST_PATH = MANIFESTS_DIR / \"gait_manifest.csv\"\n",
    "\n",
    "def build_simple_gait_manifest(gait_dir: Path) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    # scan train split only for supervised training\n",
    "    train_dir = gait_dir / \"train\"\n",
    "    if not train_dir.exists():\n",
    "        # fallback: scan all CSVs under gait_dir\n",
    "        scan_dirs = [gait_dir]\n",
    "    else:\n",
    "        scan_dirs = [train_dir]\n",
    "    for root in scan_dirs:\n",
    "        for p in root.rglob(\"*.csv\"):\n",
    "            # skip metadata-ish files\n",
    "            if p.name.lower() in {\"subjects.csv\",\"tasks.csv\",\"events.csv\",\"daily_metadata.csv\",\n",
    "                                  \"tdcsfog_metadata.csv\",\"defog_metadata.csv\",\"sample_submission.csv\"}:\n",
    "                continue\n",
    "            split = \"train\" if \"train\" in str(p).replace(\"\\\\\",\"/\").lower() else \"unknown\"\n",
    "            source = Path(p).parent.name  # e.g., defog/tdcsfog/notype\n",
    "            rows.append({\n",
    "                \"path\": str(p),\n",
    "                \"recording_id\": p.stem,\n",
    "                \"split\": split,\n",
    "                \"source\": source,\n",
    "            })\n",
    "    df = pd.DataFrame(rows).drop_duplicates(subset=[\"path\"]).reset_index(drop=True)\n",
    "    # No subject_id available? default to recording_id (grouping will still prevent leakage per recording)\n",
    "    if \"subject_id\" not in df.columns:\n",
    "        df[\"subject_id\"] = df[\"recording_id\"]\n",
    "    return df\n",
    "\n",
    "if MANIFEST_PATH.exists():\n",
    "    gman = pd.read_csv(MANIFEST_PATH)\n",
    "    print(f\"[manifest] Loaded -> {MANIFEST_PATH} (rows={len(gman)})\")\n",
    "else:\n",
    "    gman = build_simple_gait_manifest(GAIT_DIR)\n",
    "    gman.to_csv(MANIFEST_PATH, index=False)\n",
    "    print(f\"[manifest] Built -> {MANIFEST_PATH} (rows={len(gman)})\")\n",
    "\n",
    "# Restrict to train split and records that actually exist\n",
    "gman = gman[gman[\"split\"]==\"train\"].copy()\n",
    "gman = gman[gman[\"path\"].apply(lambda s: Path(str(s)).is_file())].reset_index(drop=True)\n",
    "print(\"[manifest] train rows:\", len(gman))\n",
    "gman.head(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1835ea4f",
   "metadata": {},
   "source": [
    "## Events (`events.csv`) normalization\n",
    "Your file uses columns like `id`, `init`, `completion`. We'll map them to a unified schema:\n",
    "- `Id` — recording identifier (matched to CSV file stem)\n",
    "- `start_time` — event start (seconds or samples)\n",
    "- `end_time` — event end (seconds or samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "596f6405",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[events] Using -> C:\\Users\\muham\\_Projects\\PD New\\data\\gait\\events.csv\n",
      "[events] rows=3544 | unique Ids=535 | unmatched IDs=0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>start_time</th>\n",
       "      <th>end_time</th>\n",
       "      <th>type</th>\n",
       "      <th>kinetic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>003f117e14</td>\n",
       "      <td>8.61312</td>\n",
       "      <td>14.7731</td>\n",
       "      <td>Turn</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>009ee11563</td>\n",
       "      <td>11.38470</td>\n",
       "      <td>41.1847</td>\n",
       "      <td>Turn</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>009ee11563</td>\n",
       "      <td>54.66470</td>\n",
       "      <td>58.7847</td>\n",
       "      <td>Turn</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Id  start_time  end_time  type  kinetic\n",
       "0  003f117e14     8.61312   14.7731  Turn      1.0\n",
       "1  009ee11563    11.38470   41.1847  Turn      1.0\n",
       "2  009ee11563    54.66470   58.7847  Turn      1.0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Load & normalize events.csv (supports columns: id/init/completion) ---\n",
    "import pandas as pd\n",
    "\n",
    "# Try common locations\n",
    "candidates = [\n",
    "    GAIT_DIR / \"events.csv\",\n",
    "    GAIT_DIR.parent / \"gait\" / \"events.csv\",\n",
    "    GAIT_DIR.parent.parent / \"gait\" / \"events.csv\",\n",
    "]\n",
    "events = None\n",
    "for p in candidates:\n",
    "    if p.is_file():\n",
    "        events = pd.read_csv(p)\n",
    "        print(\"[events] Using ->\", p)\n",
    "        break\n",
    "if events is None:\n",
    "    # Last-resort: search upward\n",
    "    found = list(GAIT_DIR.parent.rglob(\"events.csv\"))\n",
    "    if found:\n",
    "        events = pd.read_csv(found[0])\n",
    "        print(\"[events] Using (found) ->\", found[0])\n",
    "\n",
    "if events is None:\n",
    "    raise FileNotFoundError(\"events.csv not found. Place it under the gait folder.\")\n",
    "\n",
    "ev = events.copy()\n",
    "ev.columns = [c.strip().lower() for c in ev.columns]\n",
    "\n",
    "# Map different schemas to unified names\n",
    "rename_map = {}\n",
    "if \"id\" in ev.columns:          rename_map[\"id\"] = \"Id\"\n",
    "if \"recording_id\" in ev.columns:rename_map[\"recording_id\"] = \"Id\"\n",
    "if \"series_id\" in ev.columns:   rename_map[\"series_id\"] = \"Id\"\n",
    "\n",
    "if \"init\" in ev.columns:        rename_map[\"init\"] = \"start_time\"\n",
    "if \"start_time\" in ev.columns:  rename_map[\"start_time\"] = \"start_time\"\n",
    "if \"start\" in ev.columns:       rename_map[\"start\"] = \"start_time\"\n",
    "if \"begin\" in ev.columns:       rename_map[\"begin\"] = \"start_time\"\n",
    "\n",
    "if \"completion\" in ev.columns:  rename_map[\"completion\"] = \"end_time\"\n",
    "if \"end_time\" in ev.columns:    rename_map[\"end_time\"] = \"end_time\"\n",
    "if \"end\" in ev.columns:         rename_map[\"end\"] = \"end_time\"\n",
    "if \"stop\" in ev.columns:        rename_map[\"stop\"] = \"end_time\"\n",
    "\n",
    "ev = ev.rename(columns=rename_map)\n",
    "\n",
    "required = {\"Id\",\"start_time\",\"end_time\"}\n",
    "missing = required - set(ev.columns)\n",
    "if missing:\n",
    "    raise RuntimeError(f\"events.csv missing columns {missing}. Found columns: {list(ev.columns)}\")\n",
    "\n",
    "# Coerce numeric and clean\n",
    "ev[\"start_time\"] = pd.to_numeric(ev[\"start_time\"], errors=\"coerce\")\n",
    "ev[\"end_time\"]   = pd.to_numeric(ev[\"end_time\"], errors=\"coerce\")\n",
    "ev = ev.dropna(subset=[\"start_time\",\"end_time\"]).reset_index(drop=True)\n",
    "ev = ev[ev[\"end_time\"] > ev[\"start_time\"]].reset_index(drop=True)\n",
    "\n",
    "# If looks like milliseconds, convert to seconds\n",
    "if float(ev[[\"start_time\",\"end_time\"]].max().max()) > 1e4:\n",
    "    ev[[\"start_time\",\"end_time\"]] = ev[[\"start_time\",\"end_time\"]] / 1000.0\n",
    "\n",
    "# Normalize Id to be comparable to CSV stems\n",
    "ev[\"Id\"] = ev[\"Id\"].astype(str).str.replace(\".csv\",\"\", regex=False).str.strip()\n",
    "events = ev\n",
    "\n",
    "# Ensure manifest has recording_id as stem\n",
    "gman[\"recording_id\"] = gman[\"recording_id\"].astype(str).str.replace(\".csv\",\"\", regex=False)\n",
    "\n",
    "# Quick sanity\n",
    "from pathlib import Path as _P\n",
    "stems = set(_P(str(s)).stem for s in gman[\"path\"].astype(str))\n",
    "unmatched = set(events[\"Id\"].unique()) - stems\n",
    "print(f\"[events] rows={len(events)} | unique Ids={events['Id'].nunique()} | unmatched IDs={len(unmatched)}\")\n",
    "events.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0a4ac78d-e7ba-4325-b8d0-fbd1f2f9952d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def safe_fs(fs, default=None):\n",
    "    \"\"\"Return a valid sampling rate (>=1 Hz).\"\"\"\n",
    "    if default is None:\n",
    "        default = float(globals().get(\"FS\", 100.0))\n",
    "    try:\n",
    "        fs = float(fs)\n",
    "    except Exception:\n",
    "        fs = np.nan\n",
    "    if not np.isfinite(fs) or fs <= 0:\n",
    "        return float(default)\n",
    "    return max(1.0, fs)\n",
    "\n",
    "def finalize_fs(fs):\n",
    "    \"\"\"Clamp fs after you infer it from timestamps.\"\"\"\n",
    "    return safe_fs(fs)\n",
    "\n",
    "def window_signal(n_samples, fs, win_s, hop_s, cap=None):\n",
    "    \"\"\"Window emitter with guards so hop is never zero.\"\"\"\n",
    "    fs = safe_fs(fs)\n",
    "    win = max(1, int(round(win_s * fs)))\n",
    "    hop = max(1, int(round(hop_s * fs)))\n",
    "    count = 0\n",
    "    for start in range(0, max(1, n_samples - win + 1), hop):\n",
    "        if cap is not None and count >= cap:\n",
    "            break\n",
    "        count += 1\n",
    "        yield start, start + win\n",
    "    # If no window got emitted but we have enough samples, emit one\n",
    "    if count == 0 and n_samples >= win:\n",
    "        yield 0, win\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81944120",
   "metadata": {},
   "source": [
    "## Fast feature extraction\n",
    "- Optional **downsampling** to `DOWN_SAMPLE_TO`\n",
    "- **Time-domain** features only (mean/std/RMS) by default\n",
    "- Caps windows per recording for speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "91f22be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import welch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def window_signal(n_samples, fs, win_s, hop_s, cap=None):\n",
    "    win = int(round(win_s*fs))\n",
    "    hop = int(round(hop_s*fs))\n",
    "    count = 0\n",
    "    for start in range(0, max(1, n_samples - win + 1), hop):\n",
    "        if cap is not None and count >= cap:\n",
    "            break\n",
    "        count += 1\n",
    "        yield start, start+win\n",
    "\n",
    "def downsample_if_needed(df, target_fs, time_col):\n",
    "    \"\"\"Round timestamps to 1/target_fs bins and average numeric columns.\"\"\"\n",
    "    if time_col is None or target_fs is None:\n",
    "        return df, None\n",
    "    tt = pd.to_numeric(df[time_col], errors=\"coerce\").to_numpy()\n",
    "    if np.isnan(tt).all() or tt.size < 2:\n",
    "        return df, None\n",
    "    dt = np.nanmedian(np.diff(tt))\n",
    "    if not (dt > 0):\n",
    "        return df, None\n",
    "    fs_now = 1.0 / dt\n",
    "    if fs_now <= target_fs + 1e-6:\n",
    "        return df, fs_now\n",
    "    bins = np.floor(tt * target_fs) / target_fs\n",
    "    g = df.groupby(bins)\n",
    "    df_ds = g.mean(numeric_only=True).reset_index(drop=True)\n",
    "    return df_ds, target_fs\n",
    "\n",
    "def extract_numeric_matrix(df):\n",
    "    num_cols = [c for c in df.columns if np.issubdtype(df[c].dtype, np.number)]\n",
    "    if not num_cols:\n",
    "        return None, []\n",
    "    num_cols = num_cols[:NUMERIC_LIMIT]\n",
    "    Xs = [df[c].to_numpy(dtype=float, copy=False) for c in num_cols]\n",
    "    Xmat = np.vstack(Xs)  # (C, T)\n",
    "    return Xmat, num_cols\n",
    "\n",
    "def summarize_time(x):\n",
    "    mean = x.mean(axis=1)\n",
    "    std  = x.std(axis=1)\n",
    "    rms  = np.sqrt((x**2).mean(axis=1))\n",
    "    return np.concatenate([mean, std, rms], axis=0)\n",
    "\n",
    "def summarize_with_freq(x, fs):\n",
    "    feats = [x.mean(axis=1), x.std(axis=1), np.sqrt((x**2).mean(axis=1))]\n",
    "    f, Pxx = welch(x, fs=fs, axis=1, nperseg=min(128, x.shape[1]))\n",
    "    def band_power(lo, hi):\n",
    "        idx = (f >= lo) & (f < hi)\n",
    "        return Pxx[:, idx].sum(axis=1)\n",
    "    for lo, hi in [(0.1,0.5),(0.5,3),(3,8),(8,20)]:\n",
    "        feats.append(band_power(lo, hi))\n",
    "    return np.concatenate(feats, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a7a92c2d-6f7a-48a3-996e-f15325870985",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def safe_fs(fs, default=None):\n",
    "    \"\"\"Clamp sampling rate to a valid value (>=1 Hz).\"\"\"\n",
    "    if default is None:\n",
    "        default = float(globals().get(\"FS\", 100.0))\n",
    "    try:\n",
    "        fs = float(fs)\n",
    "    except Exception:\n",
    "        fs = np.nan\n",
    "    if not np.isfinite(fs) or fs <= 0:\n",
    "        return float(default)\n",
    "    return max(1.0, fs)\n",
    "\n",
    "def window_signal(n_samples, fs, win_s, hop_s, cap=None):\n",
    "    \"\"\"Emit [start, end) indices with guarded hop so it never becomes 0.\"\"\"\n",
    "    fs = safe_fs(fs)\n",
    "    win = max(1, int(round(win_s * fs)))\n",
    "    hop = max(1, int(round(hop_s * fs)))\n",
    "    count = 0\n",
    "    for start in range(0, max(1, n_samples - win + 1), hop):\n",
    "        if cap is not None and count >= cap:\n",
    "            break\n",
    "        count += 1\n",
    "        yield start, start + win\n",
    "    # If nothing emitted but long enough for one window, emit one\n",
    "    if count == 0 and n_samples >= win:\n",
    "        yield 0, win\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f1148251-3d87-41e6-b337-472bbbc2846d",
   "metadata": {},
   "outputs": [],
   "source": [
    "FEAT_DIM = None          # will lock after first window\n",
    "PAD_VALUE = np.nan       # or 0.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ced9d0",
   "metadata": {},
   "source": [
    "## Build dataset (X, y, groups)\n",
    "Labels are assigned 1 if the **center** of a window lies within any `[start_time, end_time]` of the same recording in `events.csv`.\n",
    "If your events times are in **samples**, we auto-convert them to seconds using the estimated sampling rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7ee0aa35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Recordings:   8%|▊         | 9/120 [00:02<00:36,  3.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/120] 15508c7f41.csv: 500 windows | fs=1.0Hz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Recordings:  16%|█▌        | 19/120 [00:06<00:33,  2.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20/120] 32d03020a9.csv: 500 windows | fs=1.0Hz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Recordings:  24%|██▍       | 29/120 [00:09<00:33,  2.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[30/120] 4f613ccf88.csv: 500 windows | fs=1.0Hz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Recordings:  32%|███▎      | 39/120 [00:12<00:25,  3.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[40/120] 6a20935af5.csv: 500 windows | fs=1.0Hz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Recordings:  41%|████      | 49/120 [00:16<00:24,  2.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50/120] 850748a138.csv: 500 windows | fs=1.0Hz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Recordings:  49%|████▉     | 59/120 [00:19<00:19,  3.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[60/120] a2f1a8ab76.csv: 500 windows | fs=1.0Hz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Recordings:  57%|█████▊    | 69/120 [00:23<00:17,  2.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[70/120] be9d33541d.csv: 500 windows | fs=1.0Hz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Recordings:  66%|██████▌   | 79/120 [00:26<00:14,  2.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[80/120] e1f92471b9.csv: 500 windows | fs=1.0Hz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Recordings:  74%|███████▍  | 89/120 [00:29<00:10,  2.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[90/120] f9efef91fb.csv: 500 windows | fs=1.0Hz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Recordings:  82%|████████▎ | 99/120 [00:33<00:08,  2.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100/120] 2cc3c30645.csv: 500 windows | fs=1.0Hz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Recordings:  91%|█████████ | 109/120 [00:37<00:04,  2.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[110/120] 60f28aa837.csv: 500 windows | fs=1.0Hz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Recordings:  99%|█████████▉| 119/120 [00:41<00:00,  2.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[120/120] 89e9ed32d1.csv: 500 windows | fs=1.0Hz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Recordings: 100%|██████████| 120/120 [00:42<00:00,  2.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature matrix: (60000, 21)\n",
      "Label counts: {0: 59044, 1: 956}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "rows, labels, groups = [], [], []\n",
    "\n",
    "gtrain = gman.copy()  # already restricted to train\n",
    "rec_paths = gtrain[\"path\"].astype(str).tolist()\n",
    "if FAST_MODE and MAX_RECORDINGS is not None:\n",
    "    rec_paths = rec_paths[:int(MAX_RECORDINGS)]\n",
    "\n",
    "# cache events by Id for quick lookup\n",
    "ev_by_id = {k: v.reset_index(drop=True) for k, v in events.groupby(\"Id\")}\n",
    "\n",
    "added_total = 0\n",
    "for i, path_str in enumerate(tqdm(rec_paths, desc=\"Recordings\")):\n",
    "    p = Path(path_str)\n",
    "    try:\n",
    "        df = pd.read_csv(p)\n",
    "    except Exception as e:\n",
    "        print(\"Skip\", p.name, \"->\", e, flush=True)\n",
    "        continue\n",
    "\n",
    "    # time column if present\n",
    "    time_col = next((c for c in [\"time\",\"Time\",\"t\",\"timestamp\",\"Timestamp\"] if c in df.columns), None)\n",
    "\n",
    "    # estimate fs\n",
    "    fs = FS\n",
    "    if time_col is not None:\n",
    "        tt = pd.to_numeric(df[time_col], errors=\"coerce\").to_numpy()\n",
    "        if tt.size > 1:\n",
    "            dt = np.nanmedian(np.diff(tt))\n",
    "            if dt and dt > 0:\n",
    "                fs = float(round(1.0/dt))\n",
    "\n",
    "    # optional downsample\n",
    "    if FAST_MODE and time_col is not None and DOWN_SAMPLE_TO is not None:\n",
    "        df, fs2 = downsample_if_needed(df, DOWN_SAMPLE_TO, time_col)\n",
    "        if fs2 is not None:\n",
    "            fs = fs2\n",
    "\n",
    "    Xmat, cols = extract_numeric_matrix(df)\n",
    "    if Xmat is None:  # no numeric data\n",
    "        continue\n",
    "\n",
    "    # windowing with cap\n",
    "    cap = MAX_WINDOWS_PER_REC if FAST_MODE else None\n",
    "    T = Xmat.shape[1]\n",
    "    rec_dur_s = T / float(fs) if fs else 0.0\n",
    "\n",
    "    rec_id = p.stem\n",
    "    ev_rec = ev_by_id.get(rec_id, None)\n",
    "\n",
    "    # if events likely in samples, convert to seconds\n",
    "    if ev_rec is not None and len(ev_rec):\n",
    "        et_max = float(ev_rec[\"end_time\"].max())\n",
    "        if rec_dur_s > 0 and (et_max > rec_dur_s * 1.5) and (et_max <= T * 1.5):\n",
    "            evc = ev_rec.copy()\n",
    "            evc[\"start_time\"] = evc[\"start_time\"] / float(fs)\n",
    "            evc[\"end_time\"]   = evc[\"end_time\"]   / float(fs)\n",
    "            ev_rec = evc\n",
    "\n",
    "    n_added = 0\n",
    "    for s, e in window_signal(T, fs, WIN_S, HOP_S, cap=cap):\n",
    "        xw = Xmat[:, s:e]\n",
    "        if xw.shape[1] < int(WIN_S*fs):\n",
    "            continue\n",
    "\n",
    "        feats = summarize_time(xw) if (FAST_MODE and not USE_FREQ_FEATS) else summarize_with_freq(xw, fs)\n",
    "        feats = np.asarray(feats, dtype=float).ravel()\n",
    "        \n",
    "        if FEAT_DIM is None:\n",
    "            FEAT_DIM = feats.size\n",
    "        \n",
    "        if feats.size != FEAT_DIM:\n",
    "            if feats.size < FEAT_DIM:\n",
    "                feats = np.pad(feats, (0, FEAT_DIM - feats.size), constant_values=PAD_VALUE)\n",
    "            else:\n",
    "                feats = feats[:FEAT_DIM]\n",
    "\n",
    "        # label via event center\n",
    "        center_t = (s + (e - s)/2) / float(fs)\n",
    "        yv = 0\n",
    "        if ev_rec is not None and len(ev_rec):\n",
    "            st = ev_rec[\"start_time\"].to_numpy()\n",
    "            en = ev_rec[\"end_time\"].to_numpy()\n",
    "            yv = int(((st <= center_t) & (center_t <= en)).any())\n",
    "\n",
    "        rows.append(feats)\n",
    "        labels.append(yv)\n",
    "        # group by subject (fallback to recording_id)\n",
    "        subj = gtrain.loc[gtrain[\"path\"]==path_str, \"subject_id\"].astype(str).values\n",
    "        groups.append(subj[0] if subj.size else rec_id)\n",
    "        n_added += 1\n",
    "\n",
    "    if (i+1) % max(1, PRINT_EVERY) == 0:\n",
    "        print(f\"[{i+1}/{len(rec_paths)}] {p.name}: {n_added} windows | fs={fs}Hz\", flush=True)\n",
    "\n",
    "X = np.asarray(rows, dtype=float)\n",
    "y = np.asarray(labels, dtype=int)\n",
    "groups = np.asarray(groups)\n",
    "\n",
    "print(\"Feature matrix:\", X.shape)\n",
    "print(\"Label counts:\", pd.Series(y).value_counts().to_dict())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d606e4c",
   "metadata": {},
   "source": [
    "## Train & evaluate (StratifiedGroupKFold)\n",
    "We use a simple **LogisticRegression** with standardization. Metrics: **ROC AUC** and **accuracy**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d18758e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1: AUC=0.745 ACC=0.772 (n=12000)\n",
      "Fold 2: AUC=0.832 ACC=0.616 (n=12000)\n",
      "Fold 3: AUC=0.333 ACC=0.649 (n=12000)\n",
      "Fold 4: AUC=0.777 ACC=0.692 (n=12000)\n",
      "Fold 5: AUC=0.586 ACC=0.793 (n=12000)\n",
      "\n",
      "Mean AUC=0.655 ± 0.180 | Mean ACC=0.704 ± 0.069\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "import numpy as np\n",
    "\n",
    "# Replace ±inf with NaN so the imputer can handle them\n",
    "X = np.asarray(X, dtype=float)\n",
    "X[~np.isfinite(X)] = np.nan\n",
    "\n",
    "# Sanity: require at least 2 classes\n",
    "if len(np.unique(y)) < 2:\n",
    "    raise RuntimeError(\"All windows share the same label. Check events mapping or units.\")\n",
    "\n",
    "cv = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "model = make_pipeline(\n",
    "    SimpleImputer(strategy=\"median\"),                    # ← handles NaN\n",
    "    StandardScaler(with_mean=True, with_std=True),\n",
    "    LogisticRegression(max_iter=1000, class_weight=\"balanced\", solver=\"lbfgs\")\n",
    ")\n",
    "\n",
    "aucs, accs = [], []\n",
    "for k, (tr, va) in enumerate(cv.split(X, y, groups=groups), 1):\n",
    "    model.fit(X[tr], y[tr])\n",
    "    prob = model.predict_proba(X[va])[:, 1]\n",
    "    pred = (prob >= 0.5).astype(int)\n",
    "    try:\n",
    "        auc = roc_auc_score(y[va], prob)\n",
    "    except Exception:\n",
    "        auc = np.nan\n",
    "    acc = accuracy_score(y[va], pred)\n",
    "    aucs.append(auc); accs.append(acc)\n",
    "    print(f\"Fold {k}: AUC={auc:.3f} ACC={acc:.3f} (n={len(va)})\")\n",
    "\n",
    "print(\"\\nMean AUC={:.3f} ± {:.3f} | Mean ACC={:.3f} ± {:.3f}\".format(\n",
    "    np.nanmean(aucs), np.nanstd(aucs), np.mean(accs), np.std(accs)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b2925ca5-213d-478c-b3f6-9e0aa4c2f123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1: PR-AUC=0.032  Best-F1=0.100 @ thr=0.802\n",
      "Fold 2: PR-AUC=0.204  Best-F1=0.351 @ thr=0.783\n",
      "Fold 3: PR-AUC=0.009  Best-F1=0.029 @ thr=0.090\n",
      "Fold 4: PR-AUC=0.025  Best-F1=0.058 @ thr=0.600\n",
      "Fold 5: PR-AUC=0.018  Best-F1=0.052 @ thr=0.459\n",
      "Mean PR-AUC=0.058  Mean Best-F1=0.118\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import average_precision_score, precision_recall_curve, f1_score\n",
    "\n",
    "def eval_fold(y_true, p):\n",
    "    ap = average_precision_score(y_true, p)\n",
    "    prs, rcs, th = precision_recall_curve(y_true, p)\n",
    "    f1s = 2*prs[:-1]*rcs[:-1]/(prs[:-1]+rcs[:-1]+1e-12)\n",
    "    i = f1s.argmax()\n",
    "    return ap, f1s[i], th[i]\n",
    "\n",
    "ap_list, f1_list = [], []\n",
    "for k,(tr,va) in enumerate(cv.split(X, y, groups=groups),1):\n",
    "    model.fit(X[tr], y[tr])\n",
    "    prob = model.predict_proba(X[va])[:,1]\n",
    "    ap, f1b, thb = eval_fold(y[va], prob)\n",
    "    ap_list.append(ap); f1_list.append(f1b)\n",
    "    print(f\"Fold {k}: PR-AUC={ap:.3f}  Best-F1={f1b:.3f} @ thr={thb:.3f}\")\n",
    "print(\"Mean PR-AUC={:.3f}  Mean Best-F1={:.3f}\".format(np.mean(ap_list), np.mean(f1_list)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "85e54558-0bef-48c4-925a-a2bb3cde94e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "model = make_pipeline(\n",
    "    SimpleImputer(strategy=\"median\"),\n",
    "    StandardScaler(),\n",
    "    LogisticRegressionCV(\n",
    "        Cs=np.logspace(-3,3,13),\n",
    "        cv=3,\n",
    "        scoring=\"roc_auc\",\n",
    "        class_weight=\"balanced\",\n",
    "        max_iter=3000,\n",
    "        n_jobs=-1\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c720180a-cc00-4754-93c5-a68c38b5a0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "model = make_pipeline(\n",
    "    SimpleImputer(strategy=\"median\"),\n",
    "    HistGradientBoostingClassifier(\n",
    "        max_depth=None,\n",
    "        learning_rate=0.05,\n",
    "        max_iter=300,\n",
    "        random_state=42\n",
    "    )\n",
    ")\n",
    "# When fitting, pass sample_weight to balance classes:\n",
    "#   from sklearn.utils.class_weight import compute_sample_weight\n",
    "#   w = compute_sample_weight(\"balanced\", y[tr])\n",
    "#   model.fit(X[tr], y[tr], histgradientboostingclassifier__sample_weight=w)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "622096f6-4d5f-4862-b654-20e8637c3070",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Id  start_time  end_time  type  kinetic\n",
      "0  003f117e14     8.61312   14.7731  Turn      1.0\n",
      "1  009ee11563    11.38470   41.1847  Turn      1.0\n",
      "2  009ee11563    54.66470   58.7847  Turn      1.0\n",
      "3  011322847a    28.09660   30.2966  Turn      1.0\n",
      "4  01d0fe7266    30.31840   31.8784  Turn      1.0\n",
      "Example rec: 89e9ed32d1 dur_s= 164756.0 fs= 1.0\n"
     ]
    }
   ],
   "source": [
    "print(events.head())\n",
    "print(\"Example rec:\", rec_id, \"dur_s=\", rec_dur_s, \"fs=\", fs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e102d796-f86d-4c17-a414-a5ea16aa5ded",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\muham\\AppData\\Local\\Temp\\ipykernel_33796\\1214977541.py:27: UserWarning: rec_ids not found or mismatched; falling back to groups for rec-level aggregation.\n",
      "  warnings.warn(\"rec_ids not found or mismatched; falling back to groups for rec-level aggregation.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1: win AUC=0.746 ACC=0.772 PR-AUC=0.032 Best-F1=0.099 | rec AUC=0.659 | subj AUC=0.659 (n_win=12000, n_rec=24, n_subj=24, pos_rate=0.0102)\n",
      "Fold 2: win AUC=0.828 ACC=0.621 PR-AUC=0.230 Best-F1=0.289 | rec AUC=0.552 | subj AUC=0.552 (n_win=12000, n_rec=24, n_subj=24, pos_rate=0.0307)\n",
      "Fold 3: win AUC=0.363 ACC=0.652 PR-AUC=0.009 Best-F1=0.029 | rec AUC=0.385 | subj AUC=0.385 (n_win=12000, n_rec=24, n_subj=24, pos_rate=0.0132)\n",
      "Fold 4: win AUC=0.776 ACC=0.692 PR-AUC=0.025 Best-F1=0.058 | rec AUC=0.454 | subj AUC=0.454 (n_win=12000, n_rec=24, n_subj=24, pos_rate=0.0111)\n",
      "Fold 5: win AUC=0.585 ACC=0.793 PR-AUC=0.018 Best-F1=0.052 | rec AUC=0.696 | subj AUC=0.696 (n_win=12000, n_rec=24, n_subj=24, pos_rate=0.0144)\n",
      "\n",
      "MEANS → Window:  AUC=0.660±0.169  ACC=0.706±0.067  PR-AUC=0.063  Best-F1=0.106\n",
      "Recording AUC: 0.549±0.118\n",
      "Subject   AUC: 0.549±0.118\n"
     ]
    }
   ],
   "source": [
    "# ==== Cross-validated evaluation: window / recording / subject ====\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, accuracy_score, average_precision_score,\n",
    "    precision_recall_curve\n",
    ")\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "import warnings\n",
    "\n",
    "# ---------- Safety & prep ----------\n",
    "X = np.asarray(X, dtype=float)\n",
    "y = np.asarray(y, dtype=int)\n",
    "groups = np.asarray(groups)\n",
    "\n",
    "# If rec_ids wasn't created during feature-building, fall back (not ideal but prevents crash)\n",
    "try:\n",
    "    rec_ids = np.asarray(rec_ids)\n",
    "    if rec_ids.shape[0] != X.shape[0]:\n",
    "        raise ValueError(\"rec_ids length mismatch.\")\n",
    "except Exception:\n",
    "    warnings.warn(\"rec_ids not found or mismatched; falling back to groups for rec-level aggregation.\")\n",
    "    rec_ids = groups.copy()\n",
    "\n",
    "# Replace ±inf with NaN, to be imputed\n",
    "X[~np.isfinite(X)] = np.nan\n",
    "\n",
    "# Sanity: require at least 2 classes overall\n",
    "if np.unique(y).size < 2:\n",
    "    raise RuntimeError(\"All windows share the same label. Check events mapping or units.\")\n",
    "\n",
    "# ---------- CV + model ----------\n",
    "cv = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Stronger baseline than plain LR: LR with inner CV on C, balanced classes\n",
    "model = make_pipeline(\n",
    "    SimpleImputer(strategy=\"median\"),\n",
    "    StandardScaler(),\n",
    "    LogisticRegressionCV(\n",
    "        Cs=np.logspace(-3, 3, 13),\n",
    "        cv=3,\n",
    "        scoring=\"roc_auc\",\n",
    "        class_weight=\"balanced\",\n",
    "        max_iter=3000,\n",
    "        n_jobs=-1,\n",
    "        solver=\"lbfgs\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# ---------- Helpers ----------\n",
    "def pr_summary(y_true, p):\n",
    "    ap = average_precision_score(y_true, p)\n",
    "    pr, rc, th = precision_recall_curve(y_true, p)\n",
    "    f1 = (2 * pr[:-1] * rc[:-1]) / (pr[:-1] + rc[:-1] + 1e-12)\n",
    "    best_idx = int(f1.argmax())\n",
    "    return ap, float(f1[best_idx]), float(th[best_idx])\n",
    "\n",
    "# ---------- Loop ----------\n",
    "aucs_win, accs_win = [], []\n",
    "pr_aucs_win, best_f1_win = [], []\n",
    "aucs_rec, aucs_subj = [], []\n",
    "\n",
    "for k, (tr, va) in enumerate(cv.split(X, y, groups=groups), 1):\n",
    "    Xtr, ytr = X[tr], y[tr]\n",
    "    Xva, yva = X[va], y[va]\n",
    "\n",
    "    # Fit\n",
    "    model.fit(Xtr, ytr)\n",
    "\n",
    "    # Predict probabilities\n",
    "    p = model.predict_proba(Xva)[:, 1]\n",
    "    yhat = (p >= 0.5).astype(int)\n",
    "\n",
    "    # Window-level metrics\n",
    "    try:\n",
    "        auc_w = roc_auc_score(yva, p)\n",
    "    except Exception:\n",
    "        auc_w = np.nan\n",
    "    acc_w = accuracy_score(yva, yhat)\n",
    "    ap_w, f1b_w, thr_w = pr_summary(yva, p)\n",
    "\n",
    "    aucs_win.append(auc_w); accs_win.append(acc_w)\n",
    "    pr_aucs_win.append(ap_w); best_f1_win.append(f1b_w)\n",
    "\n",
    "    # Recording-level AUC (mean prob per recording, label = any-positive window)\n",
    "    dfp = pd.DataFrame({\"rec\": rec_ids[va], \"y\": yva, \"p\": p})\n",
    "    agg_rec = dfp.groupby(\"rec\", as_index=False).agg(y=(\"y\", \"max\"), p=(\"p\", \"mean\"))\n",
    "    if agg_rec[\"y\"].nunique() > 1:\n",
    "        auc_r = roc_auc_score(agg_rec[\"y\"], agg_rec[\"p\"])\n",
    "    else:\n",
    "        auc_r = np.nan\n",
    "    aucs_rec.append(auc_r)\n",
    "\n",
    "    # Subject-level AUC (mean prob per subject, label = any-positive window)\n",
    "    dfs = pd.DataFrame({\"subj\": groups[va], \"y\": yva, \"p\": p})\n",
    "    agg_subj = dfs.groupby(\"subj\", as_index=False).agg(y=(\"y\", \"max\"), p=(\"p\", \"mean\"))\n",
    "    if agg_subj[\"y\"].nunique() > 1:\n",
    "        auc_s = roc_auc_score(agg_subj[\"y\"], agg_subj[\"p\"])\n",
    "    else:\n",
    "        auc_s = np.nan\n",
    "    aucs_subj.append(auc_s)\n",
    "\n",
    "    pos_rate = float(yva.mean())\n",
    "    print(\n",
    "        f\"Fold {k}: \"\n",
    "        f\"win AUC={auc_w:.3f} ACC={acc_w:.3f} PR-AUC={ap_w:.3f} Best-F1={f1b_w:.3f} \"\n",
    "        f\"| rec AUC={auc_r if np.isfinite(auc_r) else float('nan'):.3f} \"\n",
    "        f\"| subj AUC={auc_s if np.isfinite(auc_s) else float('nan'):.3f} \"\n",
    "        f\"(n_win={len(va)}, n_rec={len(agg_rec)}, n_subj={len(agg_subj)}, pos_rate={pos_rate:.4f})\"\n",
    "    )\n",
    "\n",
    "# ---------- Summary ----------\n",
    "print(\"\\nMEANS → \"\n",
    "      f\"Window:  AUC={np.nanmean(aucs_win):.3f}±{np.nanstd(aucs_win):.3f}  \"\n",
    "      f\"ACC={np.mean(accs_win):.3f}±{np.std(accs_win):.3f}  \"\n",
    "      f\"PR-AUC={np.mean(pr_aucs_win):.3f}  Best-F1={np.mean(best_f1_win):.3f}\\n\"\n",
    "      f\"Recording AUC: {np.nanmean(aucs_rec):.3f}±{np.nanstd(aucs_rec):.3f}\\n\"\n",
    "      f\"Subject   AUC: {np.nanmean(aucs_subj):.3f}±{np.nanstd(aucs_subj):.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eabe331b-ec9e-4c2d-9b00-d8df404637ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Parkinsons Env)",
   "language": "python",
   "name": "parkinsons_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
