{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17446750",
   "metadata": {},
   "source": [
    "# GaitNet — FOG Detection (Baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e520dbb",
   "metadata": {},
   "source": [
    "\n",
    "Window IMU CSVs into short segments, derive simple time+frequency features, and learn to detect **Freezing of Gait (FOG)**.\n",
    "- Labels are derived from `events.csv` if available (FOG intervals per recording).\n",
    "- Grouped by `subject_id` to avoid leakage.\n",
    "- Uses `HistGradientBoostingClassifier` for tabular signal features.\n",
    "> **Note:** If your event schema differs, adapt the `load_events()` mapper below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b366cc",
   "metadata": {},
   "source": [
    "## Install/verify dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7d9f88e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pandas OK\n",
      "numpy OK\n",
      "sklearn OK\n",
      "matplotlib OK\n",
      "scipy OK\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import sys, subprocess, importlib\n",
    "\n",
    "def ensure(pkg, import_name=None):\n",
    "    try:\n",
    "        importlib.import_module(import_name or pkg)\n",
    "        print(f\"{import_name or pkg} OK\")\n",
    "    except ImportError:\n",
    "        print(f\"Installing {pkg}...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", pkg])\n",
    "        importlib.import_module(import_name or pkg)\n",
    "        print(f\"{import_name or pkg} installed\")\n",
    "\n",
    "ensure(\"pandas\")\n",
    "ensure(\"numpy\")\n",
    "ensure(\"scikit-learn\", import_name=\"sklearn\")\n",
    "ensure(\"matplotlib\")\n",
    "ensure(\"scipy\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38472cc",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5450914b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GAIT: False\n",
      "FS= 100 win_s= 2.0 hop_s= 1.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "\n",
    "ROOT = Path(r\"C:\\Users\\muham\\_Projects\\PD\\data\")\n",
    "GAIT = ROOT / \"gait\"\n",
    "MANIFESTS = Path(r\"C:\\Users\\muham\\_Projects\\PD New\\manifests\")\n",
    "OUT_DIR = MANIFESTS / \"models\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "FS = 100  # Hz (change if your CSVs indicate a different rate)\n",
    "WIN_S = 2.0\n",
    "HOP_S = 1.0\n",
    "\n",
    "print(\"GAIT:\", GAIT.exists())\n",
    "print(\"FS=\", FS, \"win_s=\", WIN_S, \"hop_s=\", HOP_S)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4f779bea-bec6-4009-babe-576ed4dbb65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Speed knobs =====\n",
    "FAST_MODE = True               # quick run\n",
    "DOWN_SAMPLE_TO = 50            # Hz target if a time column exists (try 25 for faster)\n",
    "MAX_RECORDINGS = 100           # limit # of training recordings processed (raise later)\n",
    "MAX_WINDOWS_PER_REC = 400      # cap windows per recording\n",
    "USE_FREQ_FEATS = False         # time-domain stats only (faster). Set True later for accuracy.\n",
    "PRINT_EVERY = 10               # status every N recordings\n",
    "\n",
    "# Progress bar (auto installs if missing)\n",
    "try:\n",
    "    from tqdm.auto import tqdm\n",
    "except Exception:\n",
    "    import sys, subprocess\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"tqdm\"])\n",
    "    from tqdm.auto import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511cb560-eb13-4801-9d3f-0012aad3e0d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1d875547",
   "metadata": {},
   "source": [
    "## Load manifests and events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "822193da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Events loaded: False  0\n",
      "Train recordings: 970\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "gman = pd.read_csv(MANIFESTS/\"gait_manifest_splits.csv\")\n",
    "events_path = GAIT/\"events.csv\"\n",
    "tasks_path = GAIT/\"tasks.csv\"\n",
    "subjects_path = GAIT/\"subjects.csv\"\n",
    "\n",
    "def load_events(path):\n",
    "    # Try to normalize to: Id, start_time, end_time\n",
    "    df = pd.read_csv(path)\n",
    "    lower = {c.lower(): c for c in df.columns}\n",
    "    idc = lower.get(\"id\") or lower.get(\"recording_id\") or lower.get(\"series_id\") or lower.get(\"file_id\")\n",
    "    sc  = lower.get(\"start\") or lower.get(\"start_time\") or lower.get(\"onset\")\n",
    "    ec  = lower.get(\"end\") or lower.get(\"end_time\") or lower.get(\"offset\")\n",
    "    if idc is None or sc is None or ec is None:\n",
    "        print(\"WARNING: Could not find standard event columns; events disabled.\")\n",
    "        return None\n",
    "    out = df.rename(columns={idc:\"Id\", sc:\"start_time\", ec:\"end_time\"})[[\"Id\",\"start_time\",\"end_time\"]]\n",
    "    return out\n",
    "\n",
    "events = load_events(events_path) if events_path.exists() else None\n",
    "print(\"Events loaded:\", events is not None, \"rows=\" if events is not None else \"\", len(events) if events is not None else 0)\n",
    "\n",
    "# Keep only training recordings\n",
    "gtrain = gman[gman[\"split\"]==\"train\"].copy()\n",
    "print(\"Train recordings:\", len(gtrain))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8b1275df-18d5-4764-b81a-c12e5da2b3f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[events] rows: 3544 | columns: ['Id', 'start_time', 'end_time', 'type', 'kinetic']\n",
      "           Id  start_time  end_time  type  kinetic\n",
      "0  003f117e14     8.61312   14.7731  Turn      1.0\n",
      "1  009ee11563    11.38470   41.1847  Turn      1.0\n",
      "2  009ee11563    54.66470   58.7847  Turn      1.0\n"
     ]
    }
   ],
   "source": [
    "# --- Load & normalize events.csv (supports init/completion) ---\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "events_path = Path(r\"C:\\Users\\muham\\_Projects\\PD New\\data\\gait\\events.csv\")  # found path\n",
    "ev = pd.read_csv(events_path)\n",
    "\n",
    "# normalize column names\n",
    "ev.columns = [c.strip().lower() for c in ev.columns]\n",
    "\n",
    "# Map your schema -> unified schema\n",
    "rename_map = {}\n",
    "if \"id\" in ev.columns:\n",
    "    rename_map[\"id\"] = \"Id\"\n",
    "if \"init\" in ev.columns:\n",
    "    rename_map[\"init\"] = \"start_time\"\n",
    "if \"completion\" in ev.columns:\n",
    "    rename_map[\"completion\"] = \"end_time\"\n",
    "ev = ev.rename(columns=rename_map)\n",
    "\n",
    "# sanity check\n",
    "required = {\"Id\",\"start_time\",\"end_time\"}\n",
    "missing = required - set(ev.columns)\n",
    "if missing:\n",
    "    raise RuntimeError(f\"events.csv missing required columns {missing}; have: {list(ev.columns)}\")\n",
    "\n",
    "# coerce times to numeric\n",
    "ev[\"start_time\"] = pd.to_numeric(ev[\"start_time\"], errors=\"coerce\")\n",
    "ev[\"end_time\"]   = pd.to_numeric(ev[\"end_time\"], errors=\"coerce\")\n",
    "ev = ev.dropna(subset=[\"start_time\",\"end_time\"])\n",
    "ev = ev[ev[\"end_time\"] > ev[\"start_time\"]]\n",
    "\n",
    "# If looks like milliseconds, convert to seconds\n",
    "if float(ev[[\"start_time\",\"end_time\"]].max().max()) > 1e4:\n",
    "    ev[[\"start_time\",\"end_time\"]] = ev[[\"start_time\",\"end_time\"]] / 1000.0\n",
    "\n",
    "# Make Id comparable to CSV stems (strip .csv just in case)\n",
    "ev[\"Id\"] = ev[\"Id\"].astype(str).str.replace(\".csv\",\"\", regex=False).str.strip()\n",
    "events = ev.reset_index(drop=True)\n",
    "\n",
    "print(\"[events] rows:\", len(events), \"| columns:\", list(events.columns))\n",
    "print(events.head(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eeabc9e",
   "metadata": {},
   "source": [
    "## Feature extraction (windowed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a1083dd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Recordings:   9%|▉         | 9/100 [00:02<00:25,  3.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/100] 15508c7f41.csv: 400 windows | fs=1.0Hz | 2.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Recordings:  19%|█▉        | 19/100 [00:05<00:23,  3.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20/100] 32d03020a9.csv: 400 windows | fs=1.0Hz | 5.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Recordings:  29%|██▉       | 29/100 [00:08<00:21,  3.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[30/100] 4f613ccf88.csv: 400 windows | fs=1.0Hz | 8.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Recordings:  39%|███▉      | 39/100 [00:10<00:16,  3.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[40/100] 6a20935af5.csv: 400 windows | fs=1.0Hz | 11.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Recordings:  49%|████▉     | 49/100 [00:13<00:14,  3.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50/100] 850748a138.csv: 400 windows | fs=1.0Hz | 14.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Recordings:  59%|█████▉    | 59/100 [00:16<00:10,  3.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[60/100] a2f1a8ab76.csv: 400 windows | fs=1.0Hz | 17.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Recordings:  69%|██████▉   | 69/100 [00:19<00:08,  3.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[70/100] be9d33541d.csv: 400 windows | fs=1.0Hz | 19.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Recordings:  79%|███████▉  | 79/100 [00:22<00:06,  3.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[80/100] e1f92471b9.csv: 400 windows | fs=1.0Hz | 22.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Recordings:  89%|████████▉ | 89/100 [00:25<00:03,  3.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[90/100] f9efef91fb.csv: 400 windows | fs=1.0Hz | 25.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Recordings:  99%|█████████▉| 99/100 [00:26<00:00,  5.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100/100] 0330ea6680.csv: 400 windows | fs=1.0Hz | 27.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Recordings: 100%|██████████| 100/100 [00:27<00:00,  3.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature matrix: (40000, 21) | positives: 0 | negatives: 40000\n"
     ]
    }
   ],
   "source": [
    "# --- Fast windowing + feature extraction ---\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from scipy.signal import welch\n",
    "import time\n",
    "\n",
    "NUMERIC_LIMIT = 128  # limit number of channels to keep feature size manageable\n",
    "\n",
    "def window_signal(n_samples, fs, win_s, hop_s, cap=None):\n",
    "    win = int(round(win_s*fs))\n",
    "    hop = int(round(hop_s*fs))\n",
    "    count = 0\n",
    "    for start in range(0, max(1, n_samples - win + 1), hop):\n",
    "        if cap is not None and count >= cap:\n",
    "            break\n",
    "        count += 1\n",
    "        yield start, start+win\n",
    "\n",
    "def downsample_if_needed(df, target_fs, time_col):\n",
    "    \"\"\"Round timestamps to 1/target_fs bins and average numeric cols.\"\"\"\n",
    "    if time_col is None or target_fs is None:\n",
    "        return df, None\n",
    "    tt = pd.to_numeric(df[time_col], errors=\"coerce\").to_numpy()\n",
    "    if np.isnan(tt).all() or tt.size < 2:\n",
    "        return df, None\n",
    "    dt = np.nanmedian(np.diff(tt))\n",
    "    if not (dt > 0):\n",
    "        return df, None\n",
    "    fs_now = 1.0 / dt\n",
    "    if fs_now <= target_fs + 1e-6:\n",
    "        return df, fs_now\n",
    "    bins = np.floor(tt * target_fs) / target_fs\n",
    "    g = df.groupby(bins)\n",
    "    df_ds = g.mean(numeric_only=True).reset_index(drop=True)\n",
    "    return df_ds, target_fs\n",
    "\n",
    "def extract_features(df):\n",
    "    num_cols = [c for c in df.columns if np.issubdtype(df[c].dtype, np.number)]\n",
    "    if not num_cols:\n",
    "        return None, []\n",
    "    num_cols = num_cols[:NUMERIC_LIMIT]\n",
    "    Xs = [df[c].to_numpy(dtype=float, copy=False) for c in num_cols]\n",
    "    Xmat = np.vstack(Xs)  # (C, T)\n",
    "    return Xmat, num_cols\n",
    "\n",
    "def summarize_time_only(x):\n",
    "    # x: (C, W)\n",
    "    mean = x.mean(axis=1)\n",
    "    std  = x.std(axis=1)\n",
    "    rms  = np.sqrt((x**2).mean(axis=1))\n",
    "    return np.concatenate([mean, std, rms], axis=0)\n",
    "\n",
    "def summarize_with_freq(x, fs):\n",
    "    feats = [x.mean(axis=1), x.std(axis=1), np.sqrt((x**2).mean(axis=1))]\n",
    "    f, Pxx = welch(x, fs=fs, axis=1, nperseg=min(128, x.shape[1]))\n",
    "    def band_power(lo, hi):\n",
    "        idx = (f >= lo) & (f < hi)\n",
    "        return Pxx[:, idx].sum(axis=1)\n",
    "    for lo, hi in [(0.1,0.5),(0.5,3),(3,8),(8,20)]:\n",
    "        feats.append(band_power(lo, hi))\n",
    "    return np.concatenate(feats, axis=0)\n",
    "\n",
    "rows, labels, groups = [], [], []\n",
    "\n",
    "# restrict to training split and optionally cap number of recordings\n",
    "gtrain = gman[gman[\"split\"]==\"train\"].copy()\n",
    "rec_paths = gtrain[\"path\"].astype(str).tolist()\n",
    "if FAST_MODE and MAX_RECORDINGS is not None:\n",
    "    rec_paths = rec_paths[:int(MAX_RECORDINGS)]\n",
    "\n",
    "t0 = time.time()\n",
    "for i, path_str in enumerate(tqdm(rec_paths, desc=\"Recordings\")):\n",
    "    p = Path(path_str)\n",
    "    try:\n",
    "        df = pd.read_csv(p)\n",
    "    except Exception as e:\n",
    "        print(\"Skip\", p.name, \"->\", e, flush=True)\n",
    "        continue\n",
    "\n",
    "    # find a time column if present\n",
    "    time_col = next((c for c in [\"time\",\"Time\",\"t\",\"timestamp\",\"Timestamp\"] if c in df.columns), None)\n",
    "\n",
    "    # estimate sampling rate\n",
    "    fs = FS\n",
    "    if time_col is not None:\n",
    "        tt = pd.to_numeric(df[time_col], errors=\"coerce\").to_numpy()\n",
    "        if tt.size > 1:\n",
    "            dt = np.nanmedian(np.diff(tt))\n",
    "            if dt and dt > 0:\n",
    "                fs = float(round(1.0/dt))\n",
    "\n",
    "    # optional downsample for speed\n",
    "    if FAST_MODE and time_col is not None and DOWN_SAMPLE_TO is not None:\n",
    "        df, fs2 = downsample_if_needed(df, DOWN_SAMPLE_TO, time_col)\n",
    "        if fs2 is not None:\n",
    "            fs = fs2\n",
    "\n",
    "    Xmat, cols = extract_features(df)\n",
    "    if Xmat is None:\n",
    "        continue\n",
    "\n",
    "    # windowing with cap\n",
    "    cap = MAX_WINDOWS_PER_REC if FAST_MODE else None\n",
    "    added = 0\n",
    "    for s, e in window_signal(Xmat.shape[1], fs, WIN_S, HOP_S, cap=cap):\n",
    "        xw = Xmat[:, s:e]\n",
    "        if xw.shape[1] < int(WIN_S*fs):  # drop short last chunk\n",
    "            continue\n",
    "\n",
    "        if FAST_MODE and not USE_FREQ_FEATS:\n",
    "            feats = summarize_time_only(xw)\n",
    "        else:\n",
    "            feats = summarize_with_freq(xw, fs)\n",
    "\n",
    "        # label via events.csv if present (center of window inside any event)\n",
    "        y = 0\n",
    "        if 'events' in globals() and events is not None and time_col is not None:\n",
    "            rid_arr = gtrain.loc[gtrain[\"path\"]==path_str, \"recording_id\"].values\n",
    "            if rid_arr.size:\n",
    "                ev = events[events[\"Id\"]==rid_arr[0]]\n",
    "                if not ev.empty:\n",
    "                    center_t = (s + (e - s)/2) / float(fs)\n",
    "                    y = int(((ev[\"start_time\"] <= center_t) & (center_t <= ev[\"end_time\"])).any())\n",
    "\n",
    "        rows.append(feats)\n",
    "        labels.append(y)\n",
    "        # group by subject to avoid leakage\n",
    "        subj_arr = gtrain.loc[gtrain[\"path\"]==path_str, \"subject_id\"].astype(str).values\n",
    "        groups.append(subj_arr[0] if subj_arr.size else path_str)\n",
    "        added += 1\n",
    "\n",
    "    if (i+1) % max(1, PRINT_EVERY) == 0:\n",
    "        elapsed = time.time() - t0\n",
    "        print(f\"[{i+1}/{len(rec_paths)}] {p.name}: {added} windows | fs={fs}Hz | {elapsed:.1f}s\", flush=True)\n",
    "\n",
    "X = np.asarray(rows, dtype=float)\n",
    "y = np.asarray(labels, dtype=int)\n",
    "groups = np.asarray(groups)\n",
    "print(\"Feature matrix:\", X.shape, \"| positives:\", int(y.sum()), \"| negatives:\", int((y==0).sum()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96414b2",
   "metadata": {},
   "source": [
    "## Train classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "374f868c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\muham\\anaconda3\\envs\\parkinsons_env\\lib\\site-packages\\sklearn\\experimental\\enable_hist_gradient_boosting.py:18: UserWarning: Since version 1.0, it is not needed to import enable_hist_gradient_boosting anymore. HistGradientBoostingClassifier and HistGradientBoostingRegressor are now stable and can be normally imported from sklearn.ensemble.\n",
      "  warnings.warn(\n",
      "C:\\Users\\muham\\anaconda3\\envs\\parkinsons_env\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:424: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
      "  warnings.warn(\n",
      "C:\\Users\\muham\\anaconda3\\envs\\parkinsons_env\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:424: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
      "  warnings.warn(\n",
      "C:\\Users\\muham\\anaconda3\\envs\\parkinsons_env\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:424: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 AUC=nan ACC=1.000\n",
      "Fold 2 AUC=nan ACC=1.000\n",
      "Fold 3 AUC=nan ACC=1.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\muham\\anaconda3\\envs\\parkinsons_env\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:424: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
      "  warnings.warn(\n",
      "C:\\Users\\muham\\anaconda3\\envs\\parkinsons_env\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:424: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
      "  warnings.warn(\n",
      "C:\\Users\\muham\\anaconda3\\envs\\parkinsons_env\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:424: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 AUC=nan ACC=1.000\n",
      "Fold 5 AUC=nan ACC=1.000\n",
      "\\nCV AUC: nan\n",
      "CV ACC: 1.0\n",
      "Saved -> C:\\Users\\muham\\_Projects\\PD New\\manifests\\models\\gaitnet_hgb.joblib\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.experimental import enable_hist_gradient_boosting  # noqa: F401\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "import numpy as np\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "gkf = GroupKFold(n_splits=5)\n",
    "probs, trues = [], []\n",
    "\n",
    "for fold, (tr, te) in enumerate(gkf.split(X, y, groups)):\n",
    "    clf = HistGradientBoostingClassifier(max_depth=None, learning_rate=0.07, max_iter=300)\n",
    "    clf.fit(X[tr], y[tr])\n",
    "    p = clf.predict_proba(X[te])[:,1]\n",
    "    probs.append(p); trues.append(y[te])\n",
    "    print(f\"Fold {fold+1} AUC={roc_auc_score(y[te], p):.3f} ACC={accuracy_score(y[te], p>0.5):.3f}\")\n",
    "\n",
    "probs = np.concatenate(probs); trues = np.concatenate(trues)\n",
    "print(\"\\\\nCV AUC:\", roc_auc_score(trues, probs))\n",
    "print(\"CV ACC:\", accuracy_score(trues, probs>0.5))\n",
    "\n",
    "final_clf = HistGradientBoostingClassifier(max_depth=None, learning_rate=0.07, max_iter=400)\n",
    "final_clf.fit(X, y)\n",
    "joblib.dump({\"model\": final_clf, \"meta\":{\"fs\":FS,\"win_s\":WIN_S,\"hop_s\":HOP_S}}, OUT_DIR/\"gaitnet_hgb.joblib\")\n",
    "print(\"Saved ->\", OUT_DIR/\"gaitnet_hgb.joblib\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c9878f27-387f-4502-b47a-ee4cf9850944",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y counts: {0: 40000}\n",
      "unique groups: 46\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'cv' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munique groups:\u001b[39m\u001b[38;5;124m\"\u001b[39m, pd\u001b[38;5;241m.\u001b[39mSeries(groups)\u001b[38;5;241m.\u001b[39mnunique())\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Per-fold label counts (with your current CV splitter `cv`)\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, (tr, va) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[43mcv\u001b[49m\u001b[38;5;241m.\u001b[39msplit(X, y, groups\u001b[38;5;241m=\u001b[39mgroups), \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m      6\u001b[0m     yc \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mSeries(y[va])\u001b[38;5;241m.\u001b[39mvalue_counts()\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFold \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m label counts:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00myc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'cv' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np, pandas as pd\n",
    "print(\"y counts:\", pd.Series(y).value_counts(dropna=False).to_dict())\n",
    "print(\"unique groups:\", pd.Series(groups).nunique())\n",
    "# Per-fold label counts (with your current CV splitter `cv`)\n",
    "for k, (tr, va) in enumerate(cv.split(X, y, groups=groups), 1):\n",
    "    yc = pd.Series(y[va]).value_counts()\n",
    "    print(f\"Fold {k} label counts:\\n{yc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da6216f-d6d1-41d3-9d74-c43ca503271e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Parkinsons Env)",
   "language": "python",
   "name": "parkinsons_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
